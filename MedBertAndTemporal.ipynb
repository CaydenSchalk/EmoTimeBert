{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-18T07:24:59.921619799Z",
     "start_time": "2025-12-18T07:24:58.405930875Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "TRAINING = False"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cayden/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:00.078958092Z",
     "start_time": "2025-12-18T07:24:59.922630951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "empath_data = pd.read_csv(\"empatheticdialogues/train.csv\", on_bad_lines=\"skip\")\n",
    "val_empath_data = pd.read_csv(\"empatheticdialogues/valid.csv\", on_bad_lines=\"skip\")\n",
    "grouped = empath_data.groupby(\"conv_id\")\n",
    "val_grouped = val_empath_data.groupby(\"conv_id\")\n",
    "# speaker_counts = (\n",
    "#     empath_data\n",
    "#     .groupby(\"conv_id\")[\"speaker_idx\"]\n",
    "#     .nunique()\n",
    "# )\n",
    "# speaker_counts.value_counts()"
   ],
   "id": "8ecfdd537c8878b1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.241727086Z",
     "start_time": "2025-12-18T07:25:00.079836566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emotion_labels = empath_data[\"context\"].unique().tolist()\n",
    "emotion_to_id = {emotion: idx for idx, emotion in enumerate(emotion_labels)}\n",
    "\n",
    "conversations = []\n",
    "\n",
    "for conv_id, df_conv in grouped:\n",
    "    texts = df_conv[\"utterance\"].tolist()\n",
    "    # texts = (df_conv[\"prompt\"] + \"[SEP]\" + df_conv[\"utterance\"]).tolist()\n",
    "    labels = [emotion_to_id[x] for x in df_conv[\"context\"]]\n",
    "    timestamps = df_conv[\"utterance_idx\"].tolist()\n",
    "    speakers = (\n",
    "        df_conv[\"speaker_idx\"]\n",
    "        .rank(method=\"dense\")\n",
    "        .astype(int)\n",
    "        .sub(1)\n",
    "        .tolist()\n",
    "    )\n",
    "    conversations.append({\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"speakers\": speakers\n",
    "    })\n",
    "\n",
    "val_conversations = []\n",
    "\n",
    "for conv_id, df_conv in val_grouped:\n",
    "    texts = df_conv[\"utterance\"].tolist()\n",
    "    # texts = (df_conv[\"prompt\"] + \"[SEP]\" + df_conv[\"utterance\"]).tolist()\n",
    "    labels = [emotion_to_id[x] for x in df_conv[\"context\"]]\n",
    "    timestamps = df_conv[\"utterance_idx\"].tolist()\n",
    "    speakers = (\n",
    "        df_conv[\"speaker_idx\"]\n",
    "        .rank(method=\"dense\")\n",
    "        .astype(int)\n",
    "        .sub(1)\n",
    "        .tolist()\n",
    "    )\n",
    "    val_conversations.append({\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"speakers\": speakers\n",
    "    })\n"
   ],
   "id": "8033a32bfff868f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.330083351Z",
     "start_time": "2025-12-18T07:25:02.325923700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# texts = (empath_data[\"prompt\"].astype(str) + \"[SEP]\" + empath_data[\"utterance\"].astype(str)).tolist()\n",
    "# raw_labels = empath_data[\"context\"].values.tolist()\n",
    "# labels = [emotion_to_id[label] for label in raw_labels]\n",
    "# time_stamps = empath_data[\"utterance_idx\"].values.tolist()"
   ],
   "id": "cdb9dc536d37644a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.338005772Z",
     "start_time": "2025-12-18T07:25:02.330697672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class EmpatheticDialoguesDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, time_stamps, max_len=128):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "#         self.time_stamps = time_stamps\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         time_stamp = torch.tensor(self.time_stamps[idx], dtype=torch.int) - 1\n",
    "#         encoding = self.tokenizer(\n",
    "#             text,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=self.max_len,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "#             \"labels\": label,\n",
    "#             \"timestamps\": time_stamp,\n",
    "#         }\n",
    "\n",
    "class EmpatheticDialoguesDataset(Dataset):\n",
    "    def __init__(self, conversations, tokenizer, max_len=128):\n",
    "        self.conversations = conversations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conv = self.conversations[idx]\n",
    "\n",
    "        return {\n",
    "            \"texts\": conv[\"texts\"],\n",
    "            \"labels\": torch.tensor(conv[\"labels\"], dtype=torch.long),\n",
    "            \"timestamps\": torch.tensor(conv[\"timestamps\"], dtype=torch.long),\n",
    "            \"speakers\": torch.tensor(conv[\"speakers\"], dtype=torch.long)\n",
    "        }"
   ],
   "id": "143f2972107c94ce",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.344139406Z",
     "start_time": "2025-12-18T07:25:02.338643759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_conversations(batch, tokenizer, max_len=128):\n",
    "    B = len(batch)\n",
    "    T_max = max(len(item[\"texts\"]) for item in batch)\n",
    "\n",
    "    padded_texts = []\n",
    "    padded_labels = []\n",
    "    padded_timestamps = []\n",
    "    padded_speakers = []\n",
    "    utterance_mask = []\n",
    "    for item in batch:\n",
    "        texts = item[\"texts\"]\n",
    "        labels = item[\"labels\"]\n",
    "        timestamps = item[\"timestamps\"]\n",
    "        speakers = item[\"speakers\"]\n",
    "\n",
    "        pad_len = T_max - len(texts)\n",
    "\n",
    "        padded_texts.extend(texts + [\"\"] * pad_len)\n",
    "\n",
    "        utterance_mask.append(\n",
    "            torch.cat([torch.ones(len(texts)), torch.zeros(pad_len)])\n",
    "        )\n",
    "\n",
    "        padded_labels.append(\n",
    "            torch.cat([labels, torch.full((pad_len,), -1, dtype=torch.long)])\n",
    "        )\n",
    "\n",
    "        padded_timestamps.append(\n",
    "            torch.cat([timestamps, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        )\n",
    "\n",
    "        padded_speakers.append(\n",
    "            torch.cat([speakers, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        )\n",
    "\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        padded_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"],\n",
    "        \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        \"labels\": torch.stack(padded_labels),\n",
    "        \"timestamps\": torch.stack(padded_timestamps),\n",
    "        \"speakers\": torch.stack(padded_speakers),\n",
    "        \"utterance_mask\": torch.stack(utterance_mask)\n",
    "    }"
   ],
   "id": "b1918b6cfbdb4a64",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.889857790Z",
     "start_time": "2025-12-18T07:25:02.344633145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = EmpatheticDialoguesDataset(conversations, tokenizer)\n",
    "val_dataset = EmpatheticDialoguesDataset(val_conversations, tokenizer)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    # pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=lambda x: collate_conversations(x, tokenizer),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False, # future me, keep it false, helps reproduce results\n",
    "    num_workers=8,\n",
    "    #pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=lambda x: collate_conversations(x, tokenizer),\n",
    ")\n"
   ],
   "id": "4e3484c7e1ca5da7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.902260529Z",
     "start_time": "2025-12-18T07:25:02.898455109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        assert padding_mask.shape[:2] == x.shape[:2]\n",
    "\n",
    "        return self.encoder(x, src_key_padding_mask=padding_mask)\n"
   ],
   "id": "72b2a7c7bf7d26",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:02.911034281Z",
     "start_time": "2025-12-18T07:25:02.902651383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmotionalTimeBert(nn.Module):\n",
    "    def __init__(self, encoder_name, num_labels, max_time = 8, max_speakers=2):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        self.head_emotions = nn.Linear(hidden, num_labels)\n",
    "        self.time_embed = nn.Embedding(max_time + 1, hidden)\n",
    "        self.speakers_embed = nn.Embedding(max_speakers + 1, hidden)\n",
    "        self.temporal_transformer = TemporalTransformer(hidden, 2, 8, 0.1)# num_labels, hidden, False)\n",
    "\n",
    "        # pause training bert\n",
    "        # for p in self.encoder.parameters():\n",
    "        #     p.requires_grad = False\n",
    "\n",
    "        # for layer in self.encoder.encoder.layer[-2:]:\n",
    "        #     for p in layer.parameters():\n",
    "        #         p.requires_grad = True\n",
    "\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        for layer in self.encoder.encoder.layer[-4:]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, timestamps=None, speakers=None, labels=None, utterance_mask=None):\n",
    "        # bert_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        flat_mask = utterance_mask.view(-1).bool()\n",
    "        bert_output = self.encoder(\n",
    "            input_ids=input_ids[flat_mask],\n",
    "            attention_mask=attention_mask[flat_mask]\n",
    "        )\n",
    "\n",
    "        h = bert_output.last_hidden_state[:, 0, :]   # (B*T, H)\n",
    "        # B, T = timestamps.shape\n",
    "        # H = h.size(-1)\n",
    "\n",
    "        B, T = timestamps.shape\n",
    "        H = h.size(-1)\n",
    "\n",
    "        h_all = torch.zeros(\n",
    "            (B * T, H),\n",
    "            device=h.device,\n",
    "            dtype=h.dtype\n",
    "        )\n",
    "\n",
    "        h_all[flat_mask] = h\n",
    "\n",
    "        speakers = speakers + 1\n",
    "\n",
    "        h_t = h_all.view(B, T, H)\n",
    "        time_vec = self.time_embed(timestamps)\n",
    "        speakers_vec = self.speakers_embed(speakers)\n",
    "        Z = h_t + time_vec + speakers_vec\n",
    "        padding_mask = (labels == -1)# (timestamps == 0) # & (speakers == 0)\n",
    "        U = self.temporal_transformer(Z, padding_mask)\n",
    "\n",
    "        logits = self.head_emotions(U)\n",
    "        return logits"
   ],
   "id": "3a456721dc2f7134",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:03.295625785Z",
     "start_time": "2025-12-18T07:25:02.911551698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Enabled!\")\n",
    "\n",
    "model = EmotionalTimeBert(\"./medbert_4_epochs\", num_labels=len(emotion_labels)).to(device)\n",
    "\n",
    "if not TRAINING:\n",
    "    model.load_state_dict(torch.load(\"emotional_time_bert_5_to_show.pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.encoder.encoder.layer[-2:].parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.temporal_transformer.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": model.time_embed.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": model.speakers_embed.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": model.head_emotions.parameters(), \"lr\": 3e-4},\n",
    "])"
   ],
   "id": "3a512845921c6861",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./medbert_4_epochs and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 5080\n",
      "CUDA Enabled!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:03.301646062Z",
     "start_time": "2025-12-18T07:25:03.296300448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(bar):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        postfix = {}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        timestamps = batch[\"timestamps\"].to(device)\n",
    "        speakers = batch[\"speakers\"].to(device)\n",
    "        utterance_mask = batch[\"utterance_mask\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask, timestamps, speakers, labels, utterance_mask)\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        postfix[\"ED Loss\"] = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        progress_bar.set_postfix(step = \"Training\", loss=loss.item(), average=total_loss / num_batches)\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def validate_model(bar):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            timestamps = batch[\"timestamps\"].to(device)\n",
    "            speakers = batch[\"speakers\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)   # (B, T)\n",
    "            utterance_mask = batch[\"utterance_mask\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                timestamps,\n",
    "                speakers,\n",
    "                labels,\n",
    "                utterance_mask\n",
    "            )\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = labels != -1\n",
    "\n",
    "            all_preds.append(preds[mask].cpu())\n",
    "            all_labels.append(labels[mask].cpu())\n",
    "\n",
    "            bar.set_postfix(step=\"Validating\", loss=loss.item())\n",
    "\n",
    "    avg_val_loss = total_loss / num_batches\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    macro_f1 = f1_score(\n",
    "        all_labels.numpy(),\n",
    "        all_preds.numpy(),\n",
    "        average=\"macro\"\n",
    "    )\n",
    "\n",
    "    return avg_val_loss, macro_f1\n",
    "\n",
    "if TRAINING:\n",
    "    num_of_epochs = 5\n",
    "    for epoch in range(num_of_epochs):\n",
    "        progress_bar = tqdm(loader, total=len(loader))\n",
    "        avg_loss = train_model(progress_bar)\n",
    "        validate_progress = tqdm(val_loader, total=len(val_loader))\n",
    "        val_loss, val_f1 = validate_model(validate_progress)\n",
    "        print(f\"Epoch {epoch+1}: train loss = {avg_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}: val loss = {val_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}: val F1 = {val_f1:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"emotional_time_bert_{num_of_epochs}.pt\")\n",
    "\n"
   ],
   "id": "2ba4058df2574703",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:03.732387928Z",
     "start_time": "2025-12-18T07:25:03.302040797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_empath_data = pd.read_csv(\"empatheticdialogues/test.csv\", on_bad_lines=\"skip\")\n",
    "test_grouped = test_empath_data.groupby(\"conv_id\")\n",
    "test_conversations = []\n",
    "\n",
    "for conv_id, df_conv in test_grouped:\n",
    "    texts = df_conv[\"utterance\"].tolist()\n",
    "    # texts = (df_conv[\"prompt\"] + \"[SEP]\" + df_conv[\"utterance\"]).tolist()\n",
    "    labels = [emotion_to_id[x] for x in df_conv[\"context\"]]\n",
    "    timestamps = df_conv[\"utterance_idx\"].tolist()\n",
    "    speakers = (\n",
    "        df_conv[\"speaker_idx\"]\n",
    "        .rank(method=\"dense\")\n",
    "        .astype(int)\n",
    "        .sub(1)\n",
    "        .tolist()\n",
    "    )\n",
    "    test_conversations.append({\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"speakers\": speakers\n",
    "    })"
   ],
   "id": "ee8a807ea366a8e9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:03.737079599Z",
     "start_time": "2025-12-18T07:25:03.732970902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model, dataloader, device, emotion_labels=None):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Testing\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            timestamps = batch[\"timestamps\"].to(device)\n",
    "            speakers = batch[\"speakers\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            utterance_mask = batch[\"utterance_mask\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                timestamps=timestamps,\n",
    "                speakers=speakers,\n",
    "                labels=labels,\n",
    "                utterance_mask=utterance_mask\n",
    "            )\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            mask = labels != -1  # ignore padded utterances\n",
    "\n",
    "            all_preds.append(preds[mask].cpu())\n",
    "            all_labels.append(labels[mask].cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    macro_f1 = f1_score(\n",
    "        all_labels.numpy(),\n",
    "        all_preds.numpy(),\n",
    "        average=\"macro\"\n",
    "    )\n",
    "\n",
    "    print(f\"Test Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "    if emotion_labels is not None:\n",
    "        print(\"\\nPer-emotion results:\")\n",
    "        print(classification_report(\n",
    "            all_labels.numpy(),\n",
    "            all_preds.numpy(),\n",
    "            target_names=emotion_labels,\n",
    "            digits=3\n",
    "        ))\n",
    "\n",
    "    return macro_f1, all_preds, all_labels\n"
   ],
   "id": "662554b055e9775a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:03.745680779Z",
     "start_time": "2025-12-18T07:25:03.737543573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = EmpatheticDialoguesDataset(test_conversations, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    collate_fn=lambda x: collate_conversations(x, tokenizer)\n",
    ")"
   ],
   "id": "7318ab3595e75852",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:09.863443449Z",
     "start_time": "2025-12-18T07:25:03.746115582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_f1 = test_model(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    emotion_labels=emotion_labels\n",
    ")"
   ],
   "id": "9770b5294e8eb6b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/80 [00:00<?, ?it/s]/home/cayden/miniconda3/lib/python3.13/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Testing: 100%|██████████| 80/80 [00:06<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro F1: 0.4205\n",
      "\n",
      "Per-emotion results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " sentimental      0.403     0.415     0.409       205\n",
      "      afraid      0.309     0.262     0.284       164\n",
      "       proud      0.630     0.308     0.413       221\n",
      "    faithful      0.733     0.282     0.407       117\n",
      "   terrified      0.326     0.548     0.409       155\n",
      "      joyful      0.311     0.171     0.221       187\n",
      "       angry      0.252     0.149     0.188       181\n",
      "         sad      0.356     0.477     0.408       195\n",
      "     jealous      0.564     0.579     0.571       183\n",
      "    grateful      0.583     0.380     0.460       221\n",
      "    prepared      0.450     0.595     0.512       173\n",
      " embarrassed      0.584     0.642     0.612       179\n",
      "     excited      0.389     0.624     0.479       202\n",
      "     annoyed      0.385     0.616     0.474       198\n",
      "      lonely      0.545     0.772     0.639       171\n",
      "     ashamed      0.315     0.161     0.213       143\n",
      "      guilty      0.385     0.638     0.480       149\n",
      "   surprised      0.513     0.529     0.521       291\n",
      "   nostalgic      0.632     0.428     0.510       173\n",
      "   confident      0.423     0.371     0.395       170\n",
      "     furious      0.243     0.222     0.232       153\n",
      "disappointed      0.403     0.309     0.349       188\n",
      "      caring      0.395     0.672     0.498       177\n",
      "    trusting      0.409     0.456     0.431       147\n",
      "   disgusted      0.682     0.621     0.650       190\n",
      "anticipating      0.547     0.247     0.340       166\n",
      "     anxious      0.285     0.240     0.260       171\n",
      "     hopeful      0.526     0.343     0.415       175\n",
      "     content      0.606     0.465     0.526       172\n",
      "   impressed      0.533     0.404     0.460       178\n",
      "apprehensive      0.218     0.475     0.299       158\n",
      "  devastated      0.589     0.291     0.389       148\n",
      "\n",
      "    accuracy                          0.433      5701\n",
      "   macro avg      0.454     0.428     0.420      5701\n",
      "weighted avg      0.456     0.433     0.426      5701\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T07:25:09.886060762Z",
     "start_time": "2025-12-18T07:25:09.883059665Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2767ae8b628b18c5",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
