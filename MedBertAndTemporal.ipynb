{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:45.009259887Z",
     "start_time": "2026-01-30T00:26:43.535122756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from models.EmoTimeBert import EmotionalTimeBert\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from data_handling.datasets.EmpatheticDialogues import EmpatheticDialoguesDataset\n",
    "from utils.utils import train_model, validate_model, test_model, collate_conversations, create_new_run_dir, save_table_as_csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "TRAINING = False\n"
   ],
   "id": "7d4315d341d52ce7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cayden/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:45.054382082Z",
     "start_time": "2026-01-30T00:26:45.009835115Z"
    }
   },
   "cell_type": "code",
   "source": "base_save_location = create_new_run_dir(new_dir=TRAINING)",
   "id": "e078b201fdbb57b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:45.624462548Z",
     "start_time": "2026-01-30T00:26:45.056211969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full_empath_data = load_dataset(\"facebook/empathetic_dialogues\", trust_remote_code=True)\n",
    "empath_data = full_empath_data[\"train\"].to_pandas() #pd.read_csv(\"empatheticdialogues/train.csv\", on_bad_lines=\"skip\")\n",
    "val_empath_data = full_empath_data[\"validation\"].to_pandas()#pd.read_csv(\"empatheticdialogues/valid.csv\", on_bad_lines=\"skip\")\n",
    "grouped = empath_data.groupby(\"conv_id\")\n",
    "val_grouped = val_empath_data.groupby(\"conv_id\")\n",
    "# speaker_counts = (\n",
    "#     empath_data\n",
    "#     .groupby(\"conv_id\")[\"speaker_idx\"]\n",
    "#     .nunique()\n",
    "# )\n",
    "# speaker_counts.value_counts()"
   ],
   "id": "5b3e6876b0e7b8f2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:47.900433961Z",
     "start_time": "2026-01-30T00:26:45.706468828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emotion_labels = empath_data[\"context\"].unique().tolist()\n",
    "emotion_to_id = {emotion: idx for idx, emotion in enumerate(emotion_labels)}\n",
    "\n",
    "conversations = []\n",
    "\n",
    "for conv_id, df_conv in grouped:\n",
    "    texts = df_conv[\"utterance\"].tolist()\n",
    "    # texts = (df_conv[\"prompt\"] + \"[SEP]\" + df_conv[\"utterance\"]).tolist()\n",
    "    labels = [emotion_to_id[x] for x in df_conv[\"context\"]]\n",
    "    timestamps = df_conv[\"utterance_idx\"].tolist()\n",
    "    speakers = (\n",
    "        df_conv[\"speaker_idx\"]\n",
    "        .rank(method=\"dense\")\n",
    "        .astype(int)\n",
    "        .sub(1)\n",
    "        .tolist()\n",
    "    )\n",
    "    conversations.append({\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"speakers\": speakers\n",
    "    })\n",
    "\n",
    "val_conversations = []\n",
    "\n",
    "for conv_id, df_conv in val_grouped:\n",
    "    texts = df_conv[\"utterance\"].tolist()\n",
    "    # texts = (df_conv[\"prompt\"] + \"[SEP]\" + df_conv[\"utterance\"]).tolist()\n",
    "    labels = [emotion_to_id[x] for x in df_conv[\"context\"]]\n",
    "    timestamps = df_conv[\"utterance_idx\"].tolist()\n",
    "    speakers = (\n",
    "        df_conv[\"speaker_idx\"]\n",
    "        .rank(method=\"dense\")\n",
    "        .astype(int)\n",
    "        .sub(1)\n",
    "        .tolist()\n",
    "    )\n",
    "    val_conversations.append({\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"speakers\": speakers\n",
    "    })\n"
   ],
   "id": "421e332817361200",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:48.191296392Z",
     "start_time": "2026-01-30T00:26:47.951665906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = EmpatheticDialoguesDataset(conversations, tokenizer)\n",
    "val_dataset = EmpatheticDialoguesDataset(val_conversations, tokenizer)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=40,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    # pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=lambda x: collate_conversations(x, tokenizer),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=40,\n",
    "    shuffle=False, # future me, keep it false, helps reproduce results\n",
    "    num_workers=8,\n",
    "    #pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=lambda x: collate_conversations(x, tokenizer),\n",
    ")\n"
   ],
   "id": "8b78505d634dec8a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:48.619974779Z",
     "start_time": "2026-01-30T00:26:48.194419491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Enabled!\")\n",
    "\n",
    "model = EmotionalTimeBert(\"./medbert_4_epochs\", num_labels=len(emotion_labels)).to(device)\n",
    "\n",
    "if not TRAINING:\n",
    "    model.load_state_dict(torch.load(f\"{base_save_location}/model.pt\")) #  emotional_time_bert_GRU_To_Show5.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.encoder.encoder.layer[-2:].parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.temporal_transformer.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": model.time_embed.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": model.speakers_embed.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": model.head_emotions.parameters(), \"lr\": 3e-4},\n",
    "])"
   ],
   "id": "d460bfceeea1d306",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 5080\n",
      "CUDA Enabled!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:48.670553402Z",
     "start_time": "2026-01-30T00:26:48.623780471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_step_losses = []\n",
    "val_step_losses = []\n",
    "global_steps = []\n",
    "val_steps = []\n",
    "\n",
    "\n",
    "if TRAINING:\n",
    "    num_of_epochs = 5\n",
    "    step = 0\n",
    "    for epoch in range(num_of_epochs):\n",
    "        progress_bar = tqdm(loader, total=len(loader))\n",
    "        avg_loss, step = train_model(model, optimizer, device, criterion=criterion, bar=progress_bar, train_step_losses=train_step_losses, global_steps=global_steps, start_step=step)\n",
    "        validate_progress = tqdm(val_loader, total=len(val_loader))\n",
    "        val_loss, val_f1, step = validate_model(model, device, criterion=criterion, bar=validate_progress, val_step_losses=val_step_losses, val_steps=val_steps, start_step=step)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train loss = {avg_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}: val loss   = {val_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}: val F1     = {val_f1:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{base_save_location}/model.pt\")\n",
    "    save_table_as_csv(global_steps, f\"{base_save_location}/global_steps.csv\")\n",
    "    save_table_as_csv(train_step_losses, f\"{base_save_location}/train_step_losses.csv\")\n",
    "    save_table_as_csv(val_step_losses, f\"{base_save_location}/val_step_losses.csv\")\n",
    "    save_table_as_csv(val_steps, f\"{base_save_location}/val_steps.csv\")\n",
    "\n"
   ],
   "id": "a8aa3662b8da5857",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:48.718886178Z",
     "start_time": "2026-01-30T00:26:48.671712796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if TRAINING:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(global_steps, train_step_losses, alpha=0.4, label=\"Train (per batch)\")\n",
    "    plt.plot(val_steps, val_step_losses, alpha=0.8, label=\"Val (per batch)\")\n",
    "\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Per-batch Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_save_location}/Unsmoothed.png\")"
   ],
   "id": "e28e43e34a486ef9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:48.771273520Z",
     "start_time": "2026-01-30T00:26:48.720260852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ema(values, beta=0.98):\n",
    "    smoothed = []\n",
    "    avg = values[0]\n",
    "    for v in values:\n",
    "        avg = beta * avg + (1 - beta) * v\n",
    "        smoothed.append(avg)\n",
    "    return smoothed\n",
    "\n",
    "if TRAINING:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(global_steps, ema(train_step_losses), label=\"Train\")\n",
    "    plt.plot(val_steps, ema(val_step_losses), label=\"Val\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Per-step Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_save_location}/Smoothed.png\")"
   ],
   "id": "ba908e534550495",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:49.058906245Z",
     "start_time": "2026-01-30T00:26:48.772483590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_empath_data = full_empath_data[\"test\"].to_pandas()\n",
    "test_grouped = test_empath_data.groupby(\"conv_id\")\n",
    "test_conversations = []\n",
    "\n",
    "for conv_id, df_conv in test_grouped:\n",
    "    texts = df_conv[\"utterance\"].tolist()\n",
    "    # texts = (df_conv[\"prompt\"] + \"[SEP]\" + df_conv[\"utterance\"]).tolist()\n",
    "    labels = [emotion_to_id[x] for x in df_conv[\"context\"]]\n",
    "    timestamps = df_conv[\"utterance_idx\"].tolist()\n",
    "    speakers = (\n",
    "        df_conv[\"speaker_idx\"]\n",
    "        .rank(method=\"dense\")\n",
    "        .astype(int)\n",
    "        .sub(1)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    test_conversations.append({\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"speakers\": speakers\n",
    "    })"
   ],
   "id": "2a34da4227cbeda",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:49.119193174Z",
     "start_time": "2026-01-30T00:26:49.063350923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for conv_id, df_conv in test_grouped:\n",
    "    print(conv_id)\n",
    "    print(df_conv)\n",
    "    break"
   ],
   "id": "be88fda96b7f6826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit:0_conv:0\n",
      "        conv_id  utterance_idx context  \\\n",
      "0  hit:0_conv:0              1  guilty   \n",
      "1  hit:0_conv:0              2  guilty   \n",
      "2  hit:0_conv:0              3  guilty   \n",
      "3  hit:0_conv:0              4  guilty   \n",
      "4  hit:0_conv:0              5  guilty   \n",
      "\n",
      "                                              prompt  speaker_idx  \\\n",
      "0  I felt guilty when I was driving home one nigh...            0   \n",
      "1  I felt guilty when I was driving home one nigh...            1   \n",
      "2  I felt guilty when I was driving home one nigh...            0   \n",
      "3  I felt guilty when I was driving home one nigh...            1   \n",
      "4  I felt guilty when I was driving home one nigh...            0   \n",
      "\n",
      "                                           utterance     selfeval tags  \n",
      "0  Yeah about 10 years ago I had a horrifying exp...  2|2|5_5|5|5       \n",
      "1                       Did you suffer any injuries?  2|2|5_5|5|5       \n",
      "2  No I wasn't hit. It turned out they were drunk...  2|2|5_5|5|5       \n",
      "3  Why did you feel guilty? People really shouldn...  2|2|5_5|5|5       \n",
      "4  I don't know I was new to driving and hadn't e...  2|2|5_5|5|5       \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:26:49.169572623Z",
     "start_time": "2026-01-30T00:26:49.121691023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = EmpatheticDialoguesDataset(test_conversations, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    collate_fn=lambda x: collate_conversations(x, tokenizer)\n",
    ")"
   ],
   "id": "69adeeae8ad206b6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:27:02.100543956Z",
     "start_time": "2026-01-30T00:26:49.171646609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_f1 = test_model(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    emotion_labels=emotion_labels,\n",
    "    save_results=True,\n",
    "    save_dir=f\"{base_save_location}/classification_report.csv\",\n",
    ")\n",
    "\n",
    "# also look into bert's special keywords"
   ],
   "id": "25ece79283d53166",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:27:02.194676573Z",
     "start_time": "2026-01-30T00:27:02.148503372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def ablation(model, dataloader, device, ablate):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            utterance_mask = batch[\"utterance_mask\"].to(device)\n",
    "\n",
    "            timestamps = batch[\"timestamps\"].to(device)\n",
    "            speakers = batch[\"speakers\"].to(device)\n",
    "\n",
    "            if ablate == \"time_zero\":\n",
    "                timestamps = torch.zeros_like(timestamps)\n",
    "            elif ablate == \"time_shuffle\":\n",
    "                timestamps = timestamps[torch.randperm(timestamps.size(0))]\n",
    "            elif ablate == \"speaker_zero\":\n",
    "                speakers = torch.zeros_like(speakers)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids, attention_mask,\n",
    "                timestamps, speakers,\n",
    "                labels, utterance_mask\n",
    "            )\n",
    "\n",
    "            preds = logits.argmax(-1)\n",
    "            mask = labels != -1\n",
    "\n",
    "            all_preds.append(preds[mask].cpu())\n",
    "            all_labels.append(labels[mask].cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return f1_score(all_labels, all_preds, average=\"macro\")"
   ],
   "id": "741b37b79604c38a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:27:15.982836177Z",
     "start_time": "2026-01-30T00:27:02.197562684Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Normal:\", ablation(model, val_loader, device, \"none\"))",
   "id": "15bee95eb6e79079",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: 0.45721292987638296\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:27:30.135295267Z",
     "start_time": "2026-01-30T00:27:16.034426400Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Time zero:\", ablation(model, val_loader, device, \"time_zero\"))",
   "id": "21025e2b218b3ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time zero: 0.30866959758379153\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:27:44.618745235Z",
     "start_time": "2026-01-30T00:27:30.189643869Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Time shuffle:\", ablation(model, val_loader, device, \"time_shuffle\"))",
   "id": "c73528b233af0489",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time shuffle: 0.4586145607366411\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T00:27:58.120250868Z",
     "start_time": "2026-01-30T00:27:44.670199877Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Speaker zero:\", ablation(model, val_loader, device, \"speaker_zero\"))",
   "id": "2d3e534d54b90e41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker zero: 0.45293499501246337\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
